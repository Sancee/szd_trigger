#cloud-config
package_upgrade: false


write_files:

- path: /tmp/installation.sh
  content: |
    #!/bin/bash

    set -ex
    HADOOP_VERSION=2.10.0
    SPARK_VERSION=2.4.6
    SPAR_HADOOP_VERSION=2.7
    CONSUL_VERSION=1.8.0
    CONSUL_TEMPLATE_VERSION=0.25.0
    RSTUDIO_VERSION=1.3.1073-amd64


    echo "Creating SPARKUSER starts."
    adduser --disabled-password --gecos "" sparkuser
    chown -R sparkuser:sparkuser /home/sparkuser
    echo "sparkuser:lpds" | chpasswd
    echo "Creating SPARKUSER finished."


    hostnamectl set-hostname spark-master


    # Turn off unattended upgrade
    sed -i 's/APT::Periodic::Unattended-Upgrade "1";/APT::Periodic::Unattended-Upgrade "0";/g' /etc/apt/apt.conf.d/20auto-upgrades


    echo "Install requirement packages starts."
    # Wait for unattended upgrade
    while [[ `ps aufx | grep -v "grep" | grep "apt.systemd.daily" | wc -l` -gt 0 ]]; do
      echo "The unattended-upgrades are running..."
      sleep 1
    done

    export DEBIAN_FRONTEND=noninteractive
    apt-get update
    apt-get install -y openjdk-8-jdk openjdk-8-jre unzip r-base gdebi-core libpango1.0-dev libcurl4-gnutls-dev libssl-dev libxml2-dev
    echo "Install requirement packages starts."


    echo "Install HADOOP starts."
    wget -nc https://downloads.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz -O /home/sparkuser/hadoop-$HADOOP_VERSION.tar.gz
    tar -xzf /home/sparkuser/hadoop-$HADOOP_VERSION.tar.gz --directory /home/sparkuser
    mkdir /home/sparkuser/hadoop
    mv /home/sparkuser/hadoop-$HADOOP_VERSION/* /home/sparkuser/hadoop
    rm -r /home/sparkuser/hadoop-$HADOOP_VERSION.tar.gz /home/sparkuser/hadoop-$HADOOP_VERSION
    echo "Install HADOOP finished."


    echo "Install SPARK starts."
    wget -nc https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$SPAR_HADOOP_VERSION.tgz -O /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPAR_HADOOP_VERSION.tgz
    tar -zxf /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPAR_HADOOP_VERSION.tgz  --directory /home/sparkuser
    mkdir /home/sparkuser/spark
    mv /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPAR_HADOOP_VERSION/* /home/sparkuser/spark
    rm -r /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPAR_HADOOP_VERSION.tgz /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPAR_HADOOP_VERSION
    echo "Install SPARK finished."


    echo "Install CONSUL starts."
    wget -nc "https://releases.hashicorp.com/consul/"$CONSUL_VERSION"/consul_"$CONSUL_VERSION"_linux_amd64.zip" -O /home/sparkuser/consul_"$CONSUL_VERSION"_linux_amd64.zip
    unzip -q /home/sparkuser/consul_"$CONSUL_VERSION"_linux_amd64.zip -d /home/sparkuser/consul/
    wget -nc "https://releases.hashicorp.com/consul-template/"$CONSUL_TEMPLATE_VERSION"/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip" -O /home/sparkuser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip
    unzip -q /home/sparkuser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip -d /home/sparkuser/consul/
    rm /home/sparkuser/consul_"$CONSUL_VERSION"_linux_amd64.zip /home/sparkuser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip
    echo "Install CONSUL finished."


    echo "Install RSTUDIO starts."
    wget -nc "https://download2.rstudio.org/server/xenial/amd64/rstudio-server-$RSTUDIO_VERSION.deb" -O "/home/sparkuser/rstudio-server-$RSTUDIO_VERSION.deb"
    gdebi -n /home/sparkuser/rstudio-server-$RSTUDIO_VERSION.deb
    echo "Install RSTUDIO finished."


    echo -e "####################
    \e[92mInstallation DONE!!!\e[39m
    ####################"
  permissions: '755'

- path: /tmp/configuration.sh
  content: |
    #!/bin/bash

    set -ex
    MASTERIP=`hostname -I | col1`
    HOSTNAME=`hostname -s`


    echo "Configure HADOOP starts."
    touch /home/sparkuser/.bashrc
    chown sparkuser:sparkuser /home/sparkuser/.bashrc
    echo export PATH="/home/sparkuser/hadoop/bin:$PATH" >> /home/sparkuser/.bashrc
    mv /tmp/hadoop/configs/* /home/sparkuser/hadoop/etc/hadoop
    mv /tmp/hadoop/webconfigs/* /home/sparkuser/hadoop/share/hadoop/hdfs/webapps/hdfs/WEB-INF/
    echo "spark: lpds, admin" >> /home/sparkuser/hadoop/etc/hadoop/realm.properties
    echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre" >> /home/sparkuser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export HADOOP_PID_DIR=/home/sparkuser/hadoop" >> /home/sparkuser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export HADOOP_LOG_DIR=/home/sparkuser/hadoop/logs" >> /home/sparkuser/hadoop/etc/hadoop/hadoop-env.sh
    mkdir /home/sparkuser/hadoop/logs
    sed -i 's/HadoopMaster/'$MASTERIP'/g' /home/sparkuser/hadoop/etc/hadoop/core-site.xml
    sed -i 's/HadoopMaster/'$MASTERIP'/g' /home/sparkuser/hadoop/etc/hadoop/hdfs-site.xml
    touch /home/sparkuser/hadoop/etc/hadoop/dfs.exclude
    echo "$MASTERIP $HOSTNAME" >> /etc/hosts
    chown -R sparkuser:sparkuser /home/sparkuser/hadoop
    echo "Configure HADOOP finished."


    echo "Configure SPARK starts."
    cp /home/sparkuser/spark/conf/spark-env.sh.template /home/sparkuser/spark/conf/spark-env.sh
    echo export SPARK_HOME=/home/sparkuser/spark >> /home/sparkuser/.bashrc
    echo "SPARK_MASTER_HOST=$MASTERIP >> /home/sparkuser/spark/conf/spark-env.sh"
    echo "SPARK_LOCAL_IP=$MASTERIP >> /home/sparkuser/spark/conf/spark-env.sh"
    echo "SPARK_PUBLIC_DNS=$MASTERIP >> /home/sparkuser/spark/conf/spark-env.sh"
    echo "Configure SPARK ends."


    echo "Configure SPARK GUI SECURITY starts."
    wget https://gitlab.com/lpds-public/occopus-ml/raw/master/tutorials/ext_dep/spark-gui-sec.tgz?inline=false -O /home/sparkuser/spark-gui-sec.tgz
    tar -zxf /home/sparkuser/spark-gui-sec.tgz --directory /home/sparkuser
    cp /home/sparkuser/spark-gui-sec/basicAuthenticationFilter-0.0.1-SNAPSHOT.jar /home/sparkuser/spark/jars/.
    rm -r /home/sparkuser/spark-gui-sec*
    echo "spark.master.rest.enabled    false" >>/home/sparkuser/spark/conf/spark-defaults.conf
    echo "spark.ui.filters hu.sztaki.lpds.spark.authentication.BasicAuthenticationFilter" >> /home/sparkuser/spark/conf/spark-defaults.conf
    echo "spark.hu.sztaki.lpds.spark.authentication.BasicAuthenticationFilter.params username=spark,password=lpds,realm=realm" >> /home/sparkuser/spark/conf/spark-defaults.conf
    chown -R sparkuser:sparkuser /home/sparkuser/spark
    echo "Configure SPARK GUI SECURITY ends."


    su - sparkuser -c 'mkdir /home/sparkuser/consul/logs'
    su - sparkuser -c 'mkdir /home/sparkuser/consul/data'
    touch /home/sparkuser/downscale.log
    chown sparkuser:sparkuser /home/sparkuser/downscale.log


    echo "Launch CONSUL starts."
    systemctl start consul.service
    systemctl start consul-template-hosts.service
    echo "Launch CONSUL finished."


    chmod +x /tmp/downscale.sh
    while [[ `cat /etc/hosts | grep 'Consul' | wc -l` -eq 0 ]]; do
      echo "Waiting for /etc/host modification..."
      sleep 1
    done


    echo -e "#####################
    \e[92mConfiguration DONE!!!\e[39m
    #####################"
  permissions: '755'

- path: /tmp/start-services.sh
  content: |
    #!/bin/bash

    set -ex
    MASTERIP=`hostname -I | col1`


    echo "Launch HADOOP starts."
    echo 'Y' | /home/sparkuser/hadoop/bin/hdfs namenode -format hdfs_cluster
    /home/sparkuser/hadoop/sbin/hadoop-daemon.sh start namenode
    echo "Launch HADOOP finished."


    echo "Launch Spark starts."
    export SPARK_HOME=/home/sparkuser/spark
    /home/sparkuser/spark/sbin/start-master.sh
    echo "Launch Spark finished."


    echo -e "###################
    \e[92mServices STARTED!!!\e[39m
    ###################"
  permissions: '755'

- path: /tmp/hadoop/configs/hdfs-site.xml
  content: |
   <configuration>
    <property>
     <name>dfs.namenode.http-address</name>
      <value>HadoopMaster:50070</value>
    </property>
    <property>
      <name>dfs.name.dir</name>
      <value>/tmp</value>
      <final>true</final>
    </property>
    <property>
       <name>dfs.permissions</name>
       <value>false</value>
    </property>
    <property>
      <name>dfs.datanode.du.reserved</name>
      <value>500000000</value>
    </property>
    <property>
      <name>dfs.hosts.exclude</name>
      <value>/home/sparkuser/hadoop/etc/hadoop/dfs.exclude</value>
    </property>
    <property>
      <name>dfs.client.use.datanode.hostname</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.datanode.use.datanode.hostname</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
      <value>false</value>
    </property>
   </configuration>
  permissions: '644'

- path: /tmp/hadoop/configs/core-site.xml
  content: |
      <configuration>
      <property>
        <name>fs.default.name</name>
        <value>hdfs://HadoopMaster:9000</value>
      </property>
      </configuration>
  permissions: '644'

- path: /tmp/hadoop/webconfigs/web.xml
  content: |
    <?xml version="1.0" encoding="UTF-8"?>
    <web-app version="2.4" xmlns="http://java.sun.com/xml/ns/j2ee">
      <security-constraint>
          <web-resource-collection>
              <web-resource-name>Protected</web-resource-name>
              <url-pattern>/*</url-pattern>
          </web-resource-collection>
          <auth-constraint>
              <role-name>admin</role-name>
          </auth-constraint>
      </security-constraint>
      <login-config>
          <auth-method>BASIC</auth-method>
          <realm-name>realm</realm-name>
      </login-config>
    </web-app>
  permissions: '644'

- path: /tmp/hadoop/webconfigs/jetty-web.xml
  content: |
    <Configure class="org.mortbay.jetty.webapp.WebAppContext">
     <Get name="securityHandler">
      <Set name="userRealm">
        <New class="org.mortbay.jetty.security.HashUserRealm">
          <Set name="name">realm</Set>
          <Set name="config">/home/sparkuser/hadoop/etc/hadoop/realm.properties</Set>
        </New>
      </Set>
     </Get>
    </Configure>
  permissions: '644'

- path: /home/sparkuser/consul/hosts.ctmpl
  content: |
    127.0.0.1       localhost

    # The following lines are desirable for IPv6 capable hosts
    ::1     localhost ip6-localhost ip6-loopback
    ff02::1 ip6-allnodes
    ff02::2 ip6-allrouters

    # Consul managed
    {% raw %}
    {{range service "hadoop"}}
    {{.Address}} {{.Node}}{{end}}
    {% endraw %}
  permissions: '644'

- path: /home/sparkuser/consul/service.json
  content: |
    {"service": {"name": "hadoop"}}
  permissions: '644'

- path: /etc/systemd/system/consul.service
  content: |
    [Unit]
    Description=consul agent
    Requires=network-online.target
    After=network-online.target

    [Service]
    Restart=on-failure
    ExecStart=/bin/bash -c "/home/sparkuser/consul/consul agent -server -ui -bootstrap-expect 1 -data-dir=/home/sparkuser/consul/data -config-file=/home/sparkuser/consul/service.json -bind=$(hostname -I | col1) -client=$(hostname -I | col1) >> /home/sparkuser/consul/logs/consul.log 2>&1"
    ExecReload=/bin/kill -HUP $MAINPID
    KillSignal=SIGTERM

    [Install]
    WantedBy=multi-user.target
  permissions: '644'

- path: /etc/systemd/system/consul-template-hosts.service
  content: |
    [Unit]
    Description=consul for hosts file
    Requires=network-online.target
    After=network-online.target

    [Service]
    Restart=on-failure
    ExecStart=/bin/bash -c "/home/sparkuser/consul/consul-template -consul-addr $(hostname -I | col1):8500 -template \"/home/sparkuser/consul/hosts.ctmpl:/etc/hosts:/tmp/downscale.sh\" >> /home/sparkuser/consul/logs/consul-template-hosts.log 2>&1"
    ExecReload=/bin/kill -HUP $MAINPID
    KillSignal=SIGTERM

    [Install]
    WantedBy=multi-user.target
  permissions: '644'

- path: /tmp/downscale.sh
  content: |
    #!/bin/bash

    set -e
    FILE_LOCATION=/home/sparkuser/hosts
    LOG_LOCATION=/home/sparkuser/downscale.log

    exec 3>&1 4>&2
    trap 'exec 2>&4 1>&3' 0 1 2 3
    exec 1>>$LOG_LOCATION 2>&1

    if [ ! -f $FILE_LOCATION ]; then
        echo -e `date +%Y-%m-%d` `date +"%T"` "$FILE_LOCATION File does not exist... Creating.."
        cp /etc/hosts $FILE_LOCATION
        echo -e `date +%Y-%m-%d` `date +"%T"` "$FILE_LOCATION File created!"
    fi

    if [ `diff /etc/hosts $FILE_LOCATION | grep '>' | wc -l` -gt 0 ]; then
        echo -e `date +%Y-%m-%d` `date +"%T"` "Downscale detected. Restarting name node service..."
        su - sparkuser -c '/home/sparkuser/hadoop/sbin/hadoop-daemon.sh stop namenode'
        su - sparkuser -c '/home/sparkuser/hadoop/sbin/hadoop-daemon.sh start namenode'
        echo -e `date +%Y-%m-%d` `date +"%T"` "Namenode restarted!"
        cp /etc/hosts $FILE_LOCATION
    else [ `diff /etc/hosts $FILE_LOCATION | grep '<' | wc -l` -gt 0 ]
        echo -e `date +%Y-%m-%d` `date +"%T"` "Upscaling detected. Copy template file..."
        cp /etc/hosts $FILE_LOCATION
    fi
  permissions: '644'


runcmd:
- /tmp/installation.sh && /tmp/configuration.sh && su - sparkuser -c '/tmp/start-services.sh' && echo "SPARK MASTER DEPLOYMENT DONE." || echo -e "\e[91mPROBLEM OCCURED WITH THE INSTALLATION\e[39m"